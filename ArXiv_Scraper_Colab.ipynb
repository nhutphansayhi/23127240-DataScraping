{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7973a9db",
   "metadata": {},
   "source": [
    "# ðŸ“š arXiv Paper Scraper - Lab 1\n",
    "\n",
    "**MSSV:** 23127240  \n",
    "**Papers:** 2311.14685 Ä‘áº¿n 2312.00844 (5000 papers)  \n",
    "**MÃ´i trÆ°á»ng:** Google Colab CPU-only  \n",
    "\n",
    "---\n",
    "\n",
    "## LÆ°u Ã½ quan trá»ng\n",
    "\n",
    "1. **Thá»i gian:** Cháº¡y máº¥t khoáº£ng **5-12 giá»** tÃ¹y config\n",
    "2. **Káº¿t ná»‘i:** Giá»¯ tab browser má»Ÿ hoáº·c dÃ¹ng Colab Pro trÃ¡nh máº¥t káº¿t ná»‘i\n",
    "3. **Resume:** Náº¿u bá»‹ disconnect, cháº¡y láº¡i sáº½ tá»± Ä‘á»™ng resume tá»« chá»— dá»«ng\n",
    "4. **Metrics:** RAM, Disk, Time tá»± Ä‘á»™ng track\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5def85de",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 1: CÃ i packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a393b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÃ i packages cáº§n thiáº¿t\n",
    "!pip install arxiv==2.1.0 requests==2.31.0 pandas==2.0.3 psutil -q\n",
    "\n",
    "print(\"âœ… CÃ i Ä‘áº·t xong!\")\n",
    "print(\"\\nðŸ“Š Python version:\")\n",
    "!python --version\n",
    "print(\"\\nðŸ’¾ RAM available:\")\n",
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f52d9b",
   "metadata": {},
   "source": [
    "## ðŸ“¤ Step 2: Upload Source Code\n",
    "\n",
    "Upload your `23127371.zip` file (or the entire `23127371` folder zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df814405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"ðŸ“¤ Please upload your 23127371.zip file...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the uploaded filename\n",
    "zip_filename = list(uploaded.keys())[0]\n",
    "print(f\"\\nâœ… Uploaded: {zip_filename}\")\n",
    "\n",
    "# Extract\n",
    "!unzip -q {zip_filename}\n",
    "\n",
    "# Check if extraction was successful\n",
    "if os.path.exists('23127371'):\n",
    "    print(\"âœ… Source code extracted successfully!\")\n",
    "    !ls -la 23127371/\n",
    "else:\n",
    "    print(\"âŒ Error: 23127371 folder not found after extraction\")\n",
    "    print(\"Please check your zip file structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900520fc",
   "metadata": {},
   "source": [
    "## âœ… Step 3: Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2593f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"ðŸ” Verifying project structure...\\n\")\n",
    "\n",
    "required_files = [\n",
    "    '23127371/src/main.py',\n",
    "    '23127371/src/arxiv_scraper.py',\n",
    "    '23127371/src/reference_scraper.py',\n",
    "    '23127371/src/reference_scraper_optimized.py',\n",
    "    '23127371/src/bibtex_generator.py',\n",
    "    '23127371/src/utils.py',\n",
    "    '23127371/src/config.py',\n",
    "    '23127371/src/requirements.txt'\n",
    "]\n",
    "\n",
    "all_ok = True\n",
    "for f in required_files:\n",
    "    if os.path.exists(f):\n",
    "        print(f\"âœ… {f}\")\n",
    "    else:\n",
    "        print(f\"âŒ {f} MISSING!\")\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\nâœ… All required files are present!\")\n",
    "else:\n",
    "    print(\"\\nâŒ Some files are missing. Please check your zip file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a742e39",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 4: Test thá»­ (3 papers)\n",
    "\n",
    "Test xem code cháº¡y Ä‘Æ°á»£c chÆ°a (cháº¡y nhanh thÃ´i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94830aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/23127371/src\n",
    "\n",
    "print(\"ðŸ§ª Test vá»›i 3 papers...\\n\")\n",
    "!python main.py --start-ym 2311 --start-id 14685 --end-ym 2311 --end-id 14687\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Test xong! Check káº¿t quáº£ bÃªn dÆ°á»›i:\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8509ebb2",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 5: Xem káº¿t quáº£ test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a4c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "stats_file = '../23127240_data/scraping_stats.json'\n",
    "\n",
    "if os.path.exists(stats_file):\n",
    "    with open(stats_file, 'r') as f:\n",
    "        stats = json.load(f)\n",
    "    \n",
    "    print(\"ðŸ“Š TEST STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    print(json.dumps(stats, indent=2))\n",
    "    \n",
    "    # Show scraped papers\n",
    "    print(\"\\nðŸ“‚ Scraped Papers:\")\n",
    "    !ls -lh ../23127240_data/ | head -20\n",
    "else:\n",
    "    print(\"âŒ Stats file not found. Please run the test first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a054497",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 6: Cháº¡y full (5000 papers)\n",
    "\n",
    "âš ï¸ **LÆ¯U Ã:** Cháº¡y máº¥t **5-12 giá»** tÃ¹y config!\n",
    "\n",
    "- Giá»¯ tab browser má»Ÿ\n",
    "- Hoáº·c dÃ¹ng Colab Pro Ä‘á»ƒ khÃ´ng bá»‹ disconnect\n",
    "- Náº¿u bá»‹ disconnect, cháº¡y láº¡i cell nÃ y lÃ  nÃ³ tá»± resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "%cd /content/23127371/src\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸš€ STARTING FULL SCRAPING PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ðŸ“… Start Time: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ðŸ“Š Target: 5000 papers (2311.14685 to 2312.00844)\")\n",
    "print(f\"â±ï¸  Estimated Time: 6-12 hours\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nðŸ”„ Pipeline will automatically resume if interrupted\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run with default settings (full range)\n",
    "!python main.py\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… SCRAPING COMPLETED!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"â±ï¸  Total Time: {duration/3600:.2f} hours ({duration/60:.2f} minutes)\")\n",
    "print(f\"ðŸ• End Time: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bdbe47",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Step 7: Monitor Progress (Optional - Run in Parallel)\n",
    "\n",
    "Run this cell while Step 6 is running to monitor real-time progress.\n",
    "\n",
    "**Note:** Press the â¹ï¸ Stop button to stop monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7855896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "print(\"ðŸ“Š Monitoring scraper progress... (Press Stop button to stop)\\n\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Show last 30 lines of log\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"â° {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if os.path.exists('logs/scraper.log'):\n",
    "            !tail -n 30 logs/scraper.log\n",
    "        \n",
    "        # Count completed papers\n",
    "        if os.path.exists('../23127240_data'):\n",
    "            result = !find ../23127240_data -maxdepth 1 -type d | wc -l\n",
    "            count = int(result[0]) - 1  # Subtract parent directory\n",
    "            print(f\"\\nâœ… Completed papers: {count}/5000 ({count/50:.1f}%)\")\n",
    "        \n",
    "        # Show disk usage\n",
    "        if os.path.exists('../23127240_data'):\n",
    "            !du -sh ../23127240_data\n",
    "        \n",
    "        time.sleep(60)  # Update every minute\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nâ¹ï¸ Monitoring stopped\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00322fd",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 8: Xem metrics cuá»‘i cÃ¹ng (Ä‘á»ƒ ná»™p bÃ i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73027334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load stats\n",
    "stats_file = '../23127240_data/scraping_stats.json'\n",
    "\n",
    "if not os.path.exists(stats_file):\n",
    "    print(\"âŒ ChÆ°a cÃ³ file stats. Cháº¡y scraper trÆ°á»›c.\")\n",
    "else:\n",
    "    with open(stats_file, 'r') as f:\n",
    "        stats = json.load(f)\n",
    "    \n",
    "    pipeline = stats.get('pipeline', {})\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ðŸ“Š Sá» LIá»†U CHI TIáº¾T CHO BÃO CÃO LAB 1\")\n",
    "    print(\"MSSV: 23127240\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # I. DATA STATISTICS\n",
    "    print(\"\\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "    print(\"â•‘  I. CÃC Sá» LIá»†U THá»NG KÃŠ Vá»€ Dá»® LIá»†U (Data Statistics)           â•‘\")\n",
    "    print(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "    \n",
    "    # 1. Sá»‘ lÆ°á»£ng bÃ i bÃ¡o cÃ o thÃ nh cÃ´ng\n",
    "    successful = pipeline.get('successful_papers', 0)\n",
    "    total = pipeline.get('total_papers', 0)\n",
    "    failed = pipeline.get('failed_papers', 0)\n",
    "    \n",
    "    print(f\"\\n1ï¸âƒ£  Sá»‘ lÆ°á»£ng bÃ i bÃ¡o cÃ o thÃ nh cÃ´ng:\")\n",
    "    print(f\"    â†’ {successful} papers\")\n",
    "    \n",
    "    # 2. Tá»· lá»‡ thÃ nh cÃ´ng tá»•ng thá»ƒ\n",
    "    success_rate = (successful / max(1, total)) * 100\n",
    "    print(f\"\\n2ï¸âƒ£  Tá»· lá»‡ thÃ nh cÃ´ng tá»•ng thá»ƒ (Overall Success Rate):\")\n",
    "    print(f\"    â†’ {success_rate:.2f}% ({successful}/{total} papers)\")\n",
    "    \n",
    "    # 3-4. KÃ­ch thÆ°á»›c trung bÃ¬nh trÆ°á»›c vÃ  sau khi lá»c hÃ¬nh\n",
    "    sizes_before = pipeline.get('paper_sizes_before', [])\n",
    "    sizes_after = pipeline.get('paper_sizes_after', [])\n",
    "    \n",
    "    if sizes_before:\n",
    "        avg_before = sum(sizes_before) / len(sizes_before)\n",
    "        print(f\"\\n3ï¸âƒ£  KÃ­ch thÆ°á»›c bÃ i bÃ¡o trung bÃ¬nh TRÆ¯á»šC khi lá»c hÃ¬nh:\")\n",
    "        print(f\"    â†’ {avg_before:,.0f} bytes\")\n",
    "        print(f\"    â†’ {avg_before/1024:,.2f} KB\")\n",
    "        print(f\"    â†’ {avg_before/1024/1024:,.2f} MB\")\n",
    "    else:\n",
    "        print(f\"\\n3ï¸âƒ£  KÃ­ch thÆ°á»›c bÃ i bÃ¡o trung bÃ¬nh TRÆ¯á»šC khi lá»c hÃ¬nh:\")\n",
    "        print(f\"    â†’ KhÃ´ng cÃ³ dá»¯ liá»‡u\")\n",
    "    \n",
    "    if sizes_after:\n",
    "        avg_after = sum(sizes_after) / len(sizes_after)\n",
    "        print(f\"\\n4ï¸âƒ£  KÃ­ch thÆ°á»›c bÃ i bÃ¡o trung bÃ¬nh SAU khi lá»c hÃ¬nh:\")\n",
    "        print(f\"    â†’ {avg_after:,.0f} bytes\")\n",
    "        print(f\"    â†’ {avg_after/1024:,.2f} KB\")\n",
    "        print(f\"    â†’ {avg_after/1024/1024:,.2f} MB\")\n",
    "        \n",
    "        if sizes_before:\n",
    "            reduction = ((avg_before - avg_after) / avg_before) * 100\n",
    "            print(f\"    â†’ Giáº£m {reduction:.1f}% so vá»›i trÆ°á»›c khi lá»c\")\n",
    "    else:\n",
    "        print(f\"\\n4ï¸âƒ£  KÃ­ch thÆ°á»›c bÃ i bÃ¡o trung bÃ¬nh SAU khi lá»c hÃ¬nh:\")\n",
    "        print(f\"    â†’ KhÃ´ng cÃ³ dá»¯ liá»‡u\")\n",
    "    \n",
    "    # 5. Sá»‘ lÆ°á»£ng tham kháº£o trung bÃ¬nh\n",
    "    ref_counts = pipeline.get('reference_counts', [])\n",
    "    if ref_counts:\n",
    "        avg_refs = sum(ref_counts) / len(ref_counts)\n",
    "        total_refs = sum(ref_counts)\n",
    "        print(f\"\\n5ï¸âƒ£  Sá»‘ lÆ°á»£ng tham kháº£o trung bÃ¬nh (Average References per Paper):\")\n",
    "        print(f\"    â†’ {avg_refs:.2f} references/paper\")\n",
    "        print(f\"    â†’ Tá»•ng: {total_refs} references\")\n",
    "    else:\n",
    "        print(f\"\\n5ï¸âƒ£  Sá»‘ lÆ°á»£ng tham kháº£o trung bÃ¬nh:\")\n",
    "        print(f\"    â†’ KhÃ´ng cÃ³ dá»¯ liá»‡u\")\n",
    "    \n",
    "    # 6. Tá»· lá»‡ thÃ nh cÃ´ng cÃ o metadata tham kháº£o\n",
    "    ref_success = pipeline.get('reference_success_counts', [])\n",
    "    if ref_counts and ref_success:\n",
    "        total_attempted = sum(ref_counts)\n",
    "        total_successful = sum(ref_success)\n",
    "        ref_success_rate = (total_successful / max(1, total_attempted)) * 100\n",
    "        avg_success_per_paper = sum(ref_success) / len(ref_success)\n",
    "        \n",
    "        print(f\"\\n6ï¸âƒ£  Tá»· lá»‡ thÃ nh cÃ´ng cÃ o metadata tham kháº£o:\")\n",
    "        print(f\"    â†’ {ref_success_rate:.2f}% ({total_successful}/{total_attempted} references)\")\n",
    "        print(f\"    â†’ Trung bÃ¬nh {avg_success_per_paper:.2f} refs thÃ nh cÃ´ng/paper\")\n",
    "    else:\n",
    "        print(f\"\\n6ï¸âƒ£  Tá»· lá»‡ thÃ nh cÃ´ng cÃ o metadata tham kháº£o:\")\n",
    "        print(f\"    â†’ KhÃ´ng cÃ³ dá»¯ liá»‡u\")\n",
    "    \n",
    "    # II. SCRAPER PERFORMANCE METRICS\n",
    "    print(\"\\n\\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "    print(\"â•‘  II. HIá»†U NÄ‚NG Cá»¦A Bá»˜ CÃ€O (Scraper's Performance Metrics)       â•‘\")\n",
    "    print(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "    \n",
    "    print(\"\\nðŸ“ A. THá»œI GIAN CHáº Y (Running Time)\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # 8. Thá»i gian tÆ°á»ng tá»•ng thá»ƒ\n",
    "    total_runtime = pipeline.get('total_runtime', 0)\n",
    "    print(f\"\\n8ï¸âƒ£  Thá»i gian tÆ°á»ng tá»•ng thá»ƒ (Total Wall Time):\")\n",
    "    print(f\"    â†’ {total_runtime:.2f} seconds\")\n",
    "    print(f\"    â†’ {total_runtime/60:.2f} minutes\")\n",
    "    print(f\"    â†’ {total_runtime/3600:.2f} hours\")\n",
    "    \n",
    "    # 9. Thá»i gian trung bÃ¬nh cho má»—i bÃ i bÃ¡o\n",
    "    paper_runtimes = pipeline.get('paper_runtimes', [])\n",
    "    if paper_runtimes:\n",
    "        avg_time = sum(paper_runtimes) / len(paper_runtimes)\n",
    "        min_time = min(paper_runtimes)\n",
    "        max_time = max(paper_runtimes)\n",
    "        \n",
    "        print(f\"\\n9ï¸âƒ£  Thá»i gian trung bÃ¬nh Ä‘á»ƒ xá»­ lÃ½ má»—i bÃ i bÃ¡o:\")\n",
    "        print(f\"    â†’ {avg_time:.2f} seconds/paper\")\n",
    "        print(f\"    â†’ Nhanh nháº¥t: {min_time:.2f}s\")\n",
    "        print(f\"    â†’ Cháº­m nháº¥t: {max_time:.2f}s\")\n",
    "    else:\n",
    "        print(f\"\\n9ï¸âƒ£  Thá»i gian trung bÃ¬nh Ä‘á»ƒ xá»­ lÃ½ má»—i bÃ i bÃ¡o:\")\n",
    "        print(f\"    â†’ KhÃ´ng cÃ³ dá»¯ liá»‡u\")\n",
    "    \n",
    "    print(f\"\\nðŸ”Ÿ Tá»•ng thá»i gian Ä‘á»ƒ xá»­ lÃ½ má»™t bÃ i bÃ¡o:\")\n",
    "    if paper_runtimes:\n",
    "        print(f\"    â†’ Trung bÃ¬nh: {avg_time:.2f}s\")\n",
    "        print(f\"    â†’ (TÆ°Æ¡ng tá»± má»¥c 9)\")\n",
    "    else:\n",
    "        print(f\"    â†’ KhÃ´ng cÃ³ dá»¯ liá»‡u\")\n",
    "    \n",
    "    # 11. Entry discovery time (náº¿u cÃ³ track riÃªng)\n",
    "    print(f\"\\n1ï¸âƒ£1ï¸âƒ£ Tá»•ng thá»i gian cho bÆ°á»›c Entry Discovery:\")\n",
    "    print(f\"    â†’ KhÃ´ng track riÃªng (Ä‘Ã£ bao gá»“m trong total runtime)\")\n",
    "    print(f\"    â†’ Hoáº·c cÃ³ thá»ƒ estimate ~{total_runtime*0.05:.2f}s (~5% cá»§a total)\")\n",
    "    \n",
    "    print(\"\\n\\nðŸ“ B. Dáº¤U CHÃ‚N Bá»˜ NHá»š (Memory Footprint)\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # 12. RAM tá»‘i Ä‘a\n",
    "    max_ram = pipeline.get('max_ram_mb', 0)\n",
    "    print(f\"\\n1ï¸âƒ£2ï¸âƒ£ RAM tá»‘i Ä‘a Ä‘Ã£ sá»­ dá»¥ng (Maximum RAM Used):\")\n",
    "    print(f\"    â†’ {max_ram:.2f} MB\")\n",
    "    print(f\"    â†’ {max_ram/1024:.3f} GB\")\n",
    "    \n",
    "    # 13. Dung lÆ°á»£ng Ä‘Ä©a tá»‘i Ä‘a\n",
    "    max_disk = pipeline.get('max_disk_mb', 0)\n",
    "    print(f\"\\n1ï¸âƒ£3ï¸âƒ£ Dung lÆ°á»£ng Ä‘Ä©a tá»‘i Ä‘a (Maximum Disk Storage):\")\n",
    "    print(f\"    â†’ {max_disk:.2f} MB\")\n",
    "    print(f\"    â†’ {max_disk/1024:.3f} GB\")\n",
    "    \n",
    "    # 14. KÃ­ch thÆ°á»›c Ä‘áº§u ra cuá»‘i cÃ¹ng\n",
    "    final_disk = pipeline.get('final_disk_mb', 0)\n",
    "    print(f\"\\n1ï¸âƒ£4ï¸âƒ£ KÃ­ch thÆ°á»›c Ä‘áº§u ra cuá»‘i cÃ¹ng (Final Output Size):\")\n",
    "    print(f\"    â†’ {final_disk:.2f} MB\")\n",
    "    print(f\"    â†’ {final_disk/1024:.3f} GB\")\n",
    "    \n",
    "    # 15. RAM trung bÃ¬nh\n",
    "    avg_ram = pipeline.get('avg_ram_mb', 0)\n",
    "    print(f\"\\n1ï¸âƒ£5ï¸âƒ£ Má»©c tiÃªu thá»¥ RAM trung bÃ¬nh (Average RAM Consumption):\")\n",
    "    print(f\"    â†’ {avg_ram:.2f} MB\")\n",
    "    print(f\"    â†’ {avg_ram/1024:.3f} GB\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… COPY CÃC Sá» LIá»†U NÃ€Y VÃ€O BÃO CÃO!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nðŸ’¡ LÆ°u Ã½:\")\n",
    "    print(\"   â€¢ Testbed: Google Colab CPU-only\")\n",
    "    print(\"   â€¢ MSSV: 23127240\")\n",
    "    print(\"   â€¢ CÃ¡c sá»‘ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c Ä‘o trÃªn 1 Colab instance duy nháº¥t\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8826498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load performance metrics tá»« file JSON\n",
    "metrics_file = 'performance_metrics.json'\n",
    "\n",
    "if os.path.exists(metrics_file):\n",
    "    with open(metrics_file, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ðŸ“Š FINAL PERFORMANCE REPORT cho Report.docx\")\n",
    "    print(\"MSSV: 23127240\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"  I. DATA STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    data_stats = metrics['I_DATA_STATISTICS']\n",
    "    print(f\"\\n1ï¸âƒ£  Papers scraped successfully: {data_stats['1_papers_scraped_successfully']}\")\n",
    "    print(f\"2ï¸âƒ£  Overall success rate: {data_stats['2_overall_success_rate_percent']}%\")\n",
    "    print(f\"3ï¸âƒ£  Avg size BEFORE removing figures: {data_stats['3_avg_paper_size_before_removing_figures_MB']:.3f} MB\")\n",
    "    print(f\"4ï¸âƒ£  Avg size AFTER removing figures: {data_stats['4_avg_paper_size_after_removing_figures_MB']:.3f} MB\")\n",
    "    print(f\"5ï¸âƒ£  Avg references per paper: {data_stats['5_avg_references_per_paper']}\")\n",
    "    print(f\"6ï¸âƒ£  Reference metadata success rate: {data_stats['6_reference_metadata_success_rate_percent']}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"  II. SCRAPER PERFORMANCE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    perf = metrics['II_SCRAPER_PERFORMANCE']\n",
    "    \n",
    "    print(\"\\n  A. RUNNING TIME\")\n",
    "    print(\"  \" + \"-\"*66)\n",
    "    runtime = perf['A_RUNNING_TIME']\n",
    "    print(f\"  8ï¸âƒ£  Total wall time: {runtime['8_total_wall_time_hours']:.2f} hours\")\n",
    "    print(f\"  9ï¸âƒ£  Avg time per paper: {runtime['9_avg_time_per_paper_seconds']:.2f}s\")\n",
    "    \n",
    "    print(\"\\n  B. MEMORY FOOTPRINT\")\n",
    "    print(\"  \" + \"-\"*66)\n",
    "    memory = perf['B_MEMORY_FOOTPRINT']\n",
    "    print(f\"  1ï¸âƒ£2ï¸âƒ£ Maximum RAM used: {memory['12_maximum_RAM_used_GB']:.3f} GB\")\n",
    "    print(f\"  1ï¸âƒ£3ï¸âƒ£ Maximum disk storage: {memory['13_maximum_disk_storage_GB']:.3f} GB\")\n",
    "    print(f\"  1ï¸âƒ£4ï¸âƒ£ Final output size: {memory['14_final_output_storage_size_GB']:.3f} GB\")\n",
    "    print(f\"  1ï¸âƒ£5ï¸âƒ£ Average RAM consumption: {memory['15_average_RAM_consumption_GB']:.3f} GB\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… COPY CÃC Sá» LIá»†U NÃ€Y VÃ€O REPORT.DOCX!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nðŸ’¡ Testbed: Google Colab CPU-only\")\n",
    "    print(\"ðŸ’¡ MSSV: 23127240\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Export pretty JSON Ä‘á»ƒ backup\n",
    "    print(\"\\nðŸ“„ Full JSON (backup):\")\n",
    "    print(json.dumps(metrics, indent=2, ensure_ascii=False))\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ File performance_metrics.json khÃ´ng tÃ¬m tháº¥y!\")\n",
    "    print(\"ðŸ’¡ File nÃ y Ä‘Æ°á»£c táº¡o bá»Ÿi cell Export á»Ÿ trÃªn.\")\n",
    "    print(\"ðŸ’¡ Hoáº·c cháº¡y cell Export Ä‘á»ƒ táº¡o file nÃ y.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b198612f",
   "metadata": {},
   "source": [
    "## ðŸ“Š BÆ¯á»šC 7: Performance Report cho Report.docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce1a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "stats_file = '../23127240_data/scraping_stats.json'\n",
    "\n",
    "if os.path.exists(stats_file):\n",
    "    with open(stats_file, 'r') as f:\n",
    "        stats = json.load(f)\n",
    "    \n",
    "    pipeline = stats.get('pipeline', {})\n",
    "    \n",
    "    # Táº¡o dict metrics Ä‘á»ƒ export\n",
    "    report_metrics = {\n",
    "        \"MSSV\": \"23127240\",\n",
    "        \"Testbed\": \"Google Colab CPU-only\",\n",
    "        \n",
    "        \"I_DATA_STATISTICS\": {\n",
    "            \"1_papers_scraped_successfully\": pipeline.get('successful_papers', 0),\n",
    "            \"2_overall_success_rate_percent\": round((pipeline.get('successful_papers', 0) / max(1, pipeline.get('total_papers', 1))) * 100, 2),\n",
    "            \"3_avg_paper_size_before_removing_figures_bytes\": round(sum(pipeline.get('paper_sizes_before', [0])) / max(1, len(pipeline.get('paper_sizes_before', [1]))), 2),\n",
    "            \"3_avg_paper_size_before_removing_figures_MB\": round(sum(pipeline.get('paper_sizes_before', [0])) / max(1, len(pipeline.get('paper_sizes_before', [1]))) / 1024 / 1024, 3),\n",
    "            \"4_avg_paper_size_after_removing_figures_bytes\": round(sum(pipeline.get('paper_sizes_after', [0])) / max(1, len(pipeline.get('paper_sizes_after', [1]))), 2),\n",
    "            \"4_avg_paper_size_after_removing_figures_MB\": round(sum(pipeline.get('paper_sizes_after', [0])) / max(1, len(pipeline.get('paper_sizes_after', [1]))) / 1024 / 1024, 3),\n",
    "            \"5_avg_references_per_paper\": round(sum(pipeline.get('reference_counts', [0])) / max(1, len(pipeline.get('reference_counts', [1]))), 2),\n",
    "            \"5_total_references\": sum(pipeline.get('reference_counts', [])),\n",
    "            \"6_reference_metadata_success_rate_percent\": round((sum(pipeline.get('reference_success_counts', [0])) / max(1, sum(pipeline.get('reference_counts', [1])))) * 100, 2),\n",
    "        },\n",
    "        \n",
    "        \"II_SCRAPER_PERFORMANCE\": {\n",
    "            \"A_RUNNING_TIME\": {\n",
    "                \"8_total_wall_time_seconds\": round(pipeline.get('total_runtime', 0), 2),\n",
    "                \"8_total_wall_time_minutes\": round(pipeline.get('total_runtime', 0) / 60, 2),\n",
    "                \"8_total_wall_time_hours\": round(pipeline.get('total_runtime', 0) / 3600, 2),\n",
    "                \"9_avg_time_per_paper_seconds\": round(sum(pipeline.get('paper_runtimes', [0])) / max(1, len(pipeline.get('paper_runtimes', [1]))), 2),\n",
    "                \"10_total_time_to_process_one_paper_seconds\": round(sum(pipeline.get('paper_runtimes', [0])) / max(1, len(pipeline.get('paper_runtimes', [1]))), 2),\n",
    "            },\n",
    "            \n",
    "            \"B_MEMORY_FOOTPRINT\": {\n",
    "                \"12_maximum_RAM_used_MB\": round(pipeline.get('max_ram_mb', 0), 2),\n",
    "                \"12_maximum_RAM_used_GB\": round(pipeline.get('max_ram_mb', 0) / 1024, 3),\n",
    "                \"13_maximum_disk_storage_MB\": round(pipeline.get('max_disk_mb', 0), 2),\n",
    "                \"13_maximum_disk_storage_GB\": round(pipeline.get('max_disk_mb', 0) / 1024, 3),\n",
    "                \"14_final_output_storage_size_MB\": round(pipeline.get('final_disk_mb', 0), 2),\n",
    "                \"14_final_output_storage_size_GB\": round(pipeline.get('final_disk_mb', 0) / 1024, 3),\n",
    "                \"15_average_RAM_consumption_MB\": round(pipeline.get('avg_ram_mb', 0), 2),\n",
    "                \"15_average_RAM_consumption_GB\": round(pipeline.get('avg_ram_mb', 0) / 1024, 3),\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save ra file\n",
    "    output_file = '../performance_metrics.json'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report_metrics, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"âœ… ÄÃ£ export metrics ra file: performance_metrics.json\")\n",
    "    print(\"\\nðŸ“„ Ná»™i dung file:\")\n",
    "    print(\"=\"*80)\n",
    "    print(json.dumps(report_metrics, indent=2, ensure_ascii=False))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Download file\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(output_file)\n",
    "        print(\"\\nðŸ“¥ File Ä‘ang Ä‘Æ°á»£c download...\")\n",
    "    except:\n",
    "        print(\"\\nðŸ’¾ File Ä‘Ã£ lÆ°u táº¡i:\", output_file)\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ ChÆ°a cÃ³ file stats. Cháº¡y scraper trÆ°á»›c.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2343653",
   "metadata": {},
   "source": [
    "## ðŸ“„ BÆ°á»›c 8B: Export metrics ra file (dá»… copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2916d5",
   "metadata": {},
   "source": [
    "## ðŸ“Š BÆ°á»›c 8A: Sá»‘ liá»‡u CHI TIáº¾T cho Report (Copy vÃ o Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0be5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load statistics\n",
    "stats_file = '../23127240_data/scraping_stats.json'\n",
    "\n",
    "if not os.path.exists(stats_file):\n",
    "    print(\"âŒ Stats file not found. Please run the scraper first.\")\n",
    "else:\n",
    "    with open(stats_file, 'r') as f:\n",
    "        stats = json.load(f)\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"ðŸ“Š FINAL SCRAPING PERFORMANCE METRICS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nðŸ“ˆ FULL STATISTICS JSON:\")\n",
    "    print(json.dumps(stats, indent=2))\n",
    "\n",
    "    # Parse pipeline stats\n",
    "    pipeline = stats.get('pipeline', {})\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“‹ SUMMARY FOR REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Paper counts\n",
    "    total = pipeline.get('total_papers', 0)\n",
    "    successful = pipeline.get('successful_papers', 0)\n",
    "    failed = pipeline.get('failed_papers', 0)\n",
    "    success_rate = (successful / max(1, total)) * 100\n",
    "    \n",
    "    print(f\"\\nðŸ“š PAPER STATISTICS:\")\n",
    "    print(f\"  â€¢ Total papers attempted: {total}\")\n",
    "    print(f\"  â€¢ Successfully scraped: {successful}\")\n",
    "    print(f\"  â€¢ Failed: {failed}\")\n",
    "    print(f\"  â€¢ Success rate: {success_rate:.2f}%\")\n",
    "    \n",
    "    # Time metrics\n",
    "    total_runtime = pipeline.get('total_runtime', 0)\n",
    "    paper_runtimes = pipeline.get('paper_runtimes', [])\n",
    "    \n",
    "    print(f\"\\nâ±ï¸  TIME METRICS:\")\n",
    "    print(f\"  â€¢ Total wall time: {total_runtime:.2f}s ({total_runtime/60:.2f} min / {total_runtime/3600:.2f} hours)\")\n",
    "    \n",
    "    if paper_runtimes:\n",
    "        avg_time = sum(paper_runtimes) / len(paper_runtimes)\n",
    "        min_time = min(paper_runtimes)\n",
    "        max_time = max(paper_runtimes)\n",
    "        print(f\"  â€¢ Average time per paper: {avg_time:.2f}s\")\n",
    "        print(f\"  â€¢ Fastest paper: {min_time:.2f}s\")\n",
    "        print(f\"  â€¢ Slowest paper: {max_time:.2f}s\")\n",
    "    \n",
    "    # Memory metrics\n",
    "    max_ram = pipeline.get('max_ram_mb', 0)\n",
    "    avg_ram = pipeline.get('avg_ram_mb', 0)\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ MEMORY METRICS:\")\n",
    "    print(f\"  â€¢ Maximum RAM used: {max_ram:.2f} MB ({max_ram/1024:.2f} GB)\")\n",
    "    print(f\"  â€¢ Average RAM used: {avg_ram:.2f} MB ({avg_ram/1024:.2f} GB)\")\n",
    "    \n",
    "    # Disk metrics\n",
    "    max_disk = pipeline.get('max_disk_mb', 0)\n",
    "    final_disk = pipeline.get('final_disk_mb', 0)\n",
    "    \n",
    "    print(f\"\\nðŸ’¿ DISK STORAGE METRICS:\")\n",
    "    print(f\"  â€¢ Maximum disk used: {max_disk:.2f} MB ({max_disk/1024:.2f} GB)\")\n",
    "    print(f\"  â€¢ Final disk size: {final_disk:.2f} MB ({final_disk/1024:.2f} GB)\")\n",
    "    \n",
    "    # Reference metrics\n",
    "    ref_counts = pipeline.get('reference_counts', [])\n",
    "    ref_success = pipeline.get('reference_success_counts', [])\n",
    "    \n",
    "    if ref_counts:\n",
    "        avg_refs = sum(ref_counts) / len(ref_counts)\n",
    "        total_refs = sum(ref_counts)\n",
    "        print(f\"\\nðŸ“Ž REFERENCE STATISTICS:\")\n",
    "        print(f\"  â€¢ Average references per paper: {avg_refs:.2f}\")\n",
    "        print(f\"  â€¢ Total references scraped: {total_refs}\")\n",
    "    \n",
    "    if ref_success:\n",
    "        avg_success = sum(ref_success) / len(ref_success)\n",
    "        total_success = sum(ref_success)\n",
    "        print(f\"  â€¢ Average successful refs per paper: {avg_success:.2f}\")\n",
    "        print(f\"  â€¢ Total successful references: {total_success}\")\n",
    "    \n",
    "    # Size metrics\n",
    "    sizes_after = pipeline.get('paper_sizes_after', [])\n",
    "    \n",
    "    if sizes_after:\n",
    "        avg_size = sum(sizes_after) / len(sizes_after)\n",
    "        total_size = sum(sizes_after)\n",
    "        print(f\"\\nðŸ“¦ DATA SIZE METRICS:\")\n",
    "        print(f\"  â€¢ Average paper size: {avg_size/1024:.2f} KB\")\n",
    "        print(f\"  â€¢ Total data size: {total_size/1024/1024:.2f} MB ({total_size/1024/1024/1024:.2f} GB)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… Copy these metrics to your Lab Report!\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dea565",
   "metadata": {},
   "source": [
    "## ðŸ“„ Step 9: View Paper Details CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf38b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_file = '../23127240_data/paper_details.csv'\n",
    "\n",
    "if os.path.exists(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    print(\"ðŸ“„ PAPER DETAILS CSV\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š First 10 papers:\")\n",
    "    print(df.head(10).to_string())\n",
    "    \n",
    "    print(\"\\nðŸ“Š Last 10 papers:\")\n",
    "    print(df.tail(10).to_string())\n",
    "    \n",
    "    print(\"\\nðŸ“Š Success rate by status:\")\n",
    "    print(df['status'].value_counts())\n",
    "    \n",
    "    # Statistics\n",
    "    successful = df[df['status'] == 'success']\n",
    "    if len(successful) > 0:\n",
    "        print(\"\\nðŸ“Š Successful papers statistics:\")\n",
    "        print(f\"  â€¢ Average runtime: {successful['runtime_seconds'].mean():.2f}s\")\n",
    "        print(f\"  â€¢ Average references: {successful['reference_count'].mean():.2f}\")\n",
    "        print(f\"  â€¢ Average size: {successful['size_kb'].mean():.2f} KB\")\n",
    "else:\n",
    "    print(\"âŒ CSV file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca6f7d",
   "metadata": {},
   "source": [
    "## ðŸ” Step 10: Inspect Sample Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e3d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Show first paper\n",
    "sample_paper = '2311-14685'\n",
    "paper_dir = f'../23127240_data/{sample_paper}'\n",
    "\n",
    "if os.path.exists(paper_dir):\n",
    "    print(f\"ðŸ“‚ Inspecting paper: {sample_paper}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nðŸ“ Directory structure:\")\n",
    "    !find {paper_dir} -type f | head -30\n",
    "    \n",
    "    # Metadata\n",
    "    metadata_file = f'{paper_dir}/metadata.json'\n",
    "    if os.path.exists(metadata_file):\n",
    "        print(\"\\nðŸ“‹ Metadata:\")\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(json.dumps(metadata, indent=2))\n",
    "    \n",
    "    # References\n",
    "    ref_file = f'{paper_dir}/references.json'\n",
    "    if os.path.exists(ref_file):\n",
    "        print(\"\\nðŸ“Ž References (first 5):\")\n",
    "        with open(ref_file, 'r') as f:\n",
    "            refs = json.load(f)\n",
    "        print(json.dumps(refs[:5], indent=2))\n",
    "        print(f\"\\nTotal references: {len(refs)}\")\n",
    "    \n",
    "    # BibTeX\n",
    "    bib_file = f'{paper_dir}/references.bib'\n",
    "    if os.path.exists(bib_file):\n",
    "        print(\"\\nðŸ“š BibTeX (first 20 lines):\")\n",
    "        !head -20 {bib_file}\n",
    "else:\n",
    "    print(f\"âŒ Paper {sample_paper} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f537d3d1",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 11: Xem RAM vÃ  Disk usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fb25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"ðŸ’» SYSTEM RESOURCES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“Š Memory Usage:\")\n",
    "!free -h\n",
    "\n",
    "print(\"\\nðŸ’¿ Disk Usage:\")\n",
    "!df -h\n",
    "\n",
    "print(\"\\nðŸ“¦ Data Directory Size:\")\n",
    "!du -sh ../23127240_data/ 2>/dev/null || echo \"Data directory not found\"\n",
    "\n",
    "print(\"\\nðŸ“‚ Top 20 Largest Directories:\")\n",
    "!du -h ../23127240_data/ 2>/dev/null | sort -hr | head -20 || echo \"Data directory not found\"\n",
    "\n",
    "print(\"\\nðŸ“ˆ File Count:\")\n",
    "if os.path.exists('../23127240_data'):\n",
    "    result = !find ../23127240_data -type f | wc -l\n",
    "    print(f\"Total files: {result[0]}\")\n",
    "    \n",
    "    result = !find ../23127240_data -maxdepth 1 -type d | wc -l\n",
    "    paper_count = int(result[0]) - 1\n",
    "    print(f\"Total paper directories: {paper_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd132619",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 12: ÄÃ³ng gÃ³i káº¿t quáº£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "print(\"ðŸ“¦ Packaging results...\\n\")\n",
    "\n",
    "# Create zip of statistics and CSV files (for quick download)\n",
    "print(\"Creating summary package...\")\n",
    "!cd .. && zip -j 23127240_summary.zip \\\n",
    "    23127240_data/scraping_stats.json \\\n",
    "    23127240_data/scraping_stats.csv \\\n",
    "    23127240_data/paper_details.csv\n",
    "\n",
    "print(\"âœ… Summary package created: 23127240_summary.zip\")\n",
    "print(\"\\nðŸ“¥ Downloading summary files...\")\n",
    "files.download('../23127240_summary.zip')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âš ï¸  IMPORTANT: Full Data Package\")\n",
    "print(\"=\"*80)\n",
    "print(\"The full data directory is too large to download directly from Colab.\")\n",
    "print(\"Please use ONE of these methods:\\n\")\n",
    "print(\"ðŸ“¤ Method 1: Upload to Google Drive (RECOMMENDED)\")\n",
    "print(\"   Run the cell below to mount Google Drive and copy files\\n\")\n",
    "print(\"ðŸ“¤ Method 2: Download in chunks\")\n",
    "print(\"   Create zip files of smaller batches manually\\n\")\n",
    "print(\"ðŸ“¤ Method 3: Use Colab Pro\")\n",
    "print(\"   Larger download limits with Pro subscription\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaa3035",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 13: Upload data lÃªn Google Drive (QUAN TRá»ŒNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c9d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "print(\"ðŸ“ Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create destination directory\n",
    "dest_dir = '/content/drive/MyDrive/Lab1_ArXiv_Data'\n",
    "os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nðŸ“¤ Copying data to Google Drive: {dest_dir}\")\n",
    "print(\"âš ï¸  This may take 30-60 minutes depending on data size...\\n\")\n",
    "\n",
    "# Copy the entire data directory\n",
    "if os.path.exists('../23127240_data'):\n",
    "    # Copy statistics first (small files)\n",
    "    print(\"Copying statistics files...\")\n",
    "    for file in ['scraping_stats.json', 'scraping_stats.csv', 'paper_details.csv']:\n",
    "        src = f'../23127240_data/{file}'\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy2(src, dest_dir)\n",
    "            print(f\"  âœ… {file}\")\n",
    "    \n",
    "    # Copy all paper directories\n",
    "    print(\"\\nCopying paper directories...\")\n",
    "    data_dest = f'{dest_dir}/23127240_data'\n",
    "    \n",
    "    if os.path.exists(data_dest):\n",
    "        print(f\"Warning: {data_dest} already exists. Merging...\")\n",
    "    \n",
    "    !cp -r ../23127240_data {dest_dir}/\n",
    "    \n",
    "    print(\"\\nâœ… Upload to Google Drive completed!\")\n",
    "    print(f\"ðŸ“ Location: {dest_dir}\")\n",
    "    \n",
    "    # Show what was uploaded\n",
    "    !ls -lh {dest_dir}\n",
    "    !du -sh {dest_dir}\n",
    "else:\n",
    "    print(\"âŒ Data directory not found. Please run the scraper first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dc9989",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 14: Final Checklist for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c8f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ“‹ SUBMISSION CHECKLIST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "checklist = [\n",
    "    ('Data scraped', os.path.exists('../23127240_data')),\n",
    "    ('Statistics JSON', os.path.exists('../23127240_data/scraping_stats.json')),\n",
    "    ('Statistics CSV', os.path.exists('../23127240_data/scraping_stats.csv')),\n",
    "    ('Paper details CSV', os.path.exists('../23127240_data/paper_details.csv')),\n",
    "]\n",
    "\n",
    "# Check if we have expected number of papers\n",
    "if os.path.exists('../23127240_data'):\n",
    "    result = !find ../23127240_data -maxdepth 1 -type d | wc -l\n",
    "    paper_count = int(result[0]) - 1\n",
    "    checklist.append((f'Papers scraped ({paper_count}/5000)', paper_count >= 4500))  # Allow some failures\n",
    "\n",
    "print(\"\\nâœ… Required Items:\")\n",
    "for item, status in checklist:\n",
    "    icon = \"âœ…\" if status else \"âŒ\"\n",
    "    print(f\"{icon} {item}\")\n",
    "\n",
    "print(\"\\nðŸ“ Next Steps:\")\n",
    "print(\"1. âœ… Download summary statistics (already done in Step 12)\")\n",
    "print(\"2. â˜ï¸  Upload full data to Google Drive (Step 13)\")\n",
    "print(\"3. ðŸ“Š Copy performance metrics to your Lab Report\")\n",
    "print(\"4. ðŸ“¹ Record demo video (if required)\")\n",
    "print(\"5. ðŸ“¤ Submit on Moodle\")\n",
    "\n",
    "print(\"\\nðŸ“Š Quick Stats Summary:\")\n",
    "if os.path.exists('../23127240_data/scraping_stats.json'):\n",
    "    with open('../23127240_data/scraping_stats.json', 'r') as f:\n",
    "        stats = json.load(f)\n",
    "        pipeline = stats.get('pipeline', {})\n",
    "        print(f\"  â€¢ Papers scraped: {pipeline.get('successful_papers', 0)}/{pipeline.get('total_papers', 0)}\")\n",
    "        print(f\"  â€¢ Success rate: {pipeline.get('successful_papers', 0)/max(1, pipeline.get('total_papers', 0))*100:.1f}%\")\n",
    "        print(f\"  â€¢ Total time: {pipeline.get('total_runtime', 0)/3600:.2f} hours\")\n",
    "        print(f\"  â€¢ Max RAM: {pipeline.get('max_ram_mb', 0):.2f} MB\")\n",
    "        print(f\"  â€¢ Final disk: {pipeline.get('final_disk_mb', 0)/1024:.2f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ‰ CONGRATULATIONS! Your Lab 1 scraping is complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cabd66",
   "metadata": {},
   "source": [
    "## ðŸ”„ BONUS: Resume from Interruption\n",
    "\n",
    "If your Colab session was interrupted, run this cell to check progress and resume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc16d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"ðŸ” Checking current progress...\\n\")\n",
    "\n",
    "if not os.path.exists('../23127240_data'):\n",
    "    print(\"âŒ No data directory found. Starting from scratch.\")\n",
    "    print(\"   Please run Step 6 to start scraping.\")\n",
    "else:\n",
    "    # Count completed papers\n",
    "    result = !find ../23127240_data -maxdepth 1 -type d | wc -l\n",
    "    paper_count = int(result[0]) - 1\n",
    "    \n",
    "    progress = (paper_count / 5000) * 100\n",
    "    \n",
    "    print(f\"ðŸ“Š Current Progress:\")\n",
    "    print(f\"  â€¢ Papers completed: {paper_count}/5000 ({progress:.1f}%)\")\n",
    "    print(f\"  â€¢ Papers remaining: {5000 - paper_count}\")\n",
    "    \n",
    "    if paper_count >= 5000:\n",
    "        print(\"\\nâœ… All papers completed!\")\n",
    "    else:\n",
    "        print(f\"\\nðŸ”„ To resume: Just run Step 6 again.\")\n",
    "        print(f\"   The scraper will automatically skip completed papers.\")\n",
    "    \n",
    "    # Show disk usage\n",
    "    print(f\"\\nðŸ’¾ Current disk usage:\")\n",
    "    !du -sh ../23127240_data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
